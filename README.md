# Apache Airflow

The Most Important Data Orchestration Tool

- Airflow is a platform for managing, scheduling, and monitoring data pipelines.
- It allows you to create, transform, load data through simple processes or even complex processes, using a set of independent tasks that facilitates automation and scalability.
- Airflow is, without a doubt, one of the top modern data engineering tools.

### What is Apache Airflow

Airflow is a project that was created by Bianchi in 2015 and like many other opensource projects, it ended up being kept.
It was eventually passed on to the Apache Foundation and is now maintained by the Apache Foundation.
It is developed in Python and one of the features is that it is extensible.

- Its goal is to orchestrate the data pipeline.
- A dag is developed and within the dags the tasks

Within the dags, it is also developed how the precedence of the tasks will be

<div> 
<p><a href=" https://github.com/JosiTubaroski/Running_Airflow_in_Docker">Runing Airflow in Docker</a></p>
</div> 
 

